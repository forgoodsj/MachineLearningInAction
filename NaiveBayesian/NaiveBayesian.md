* ### Naive Bayesian 中文译名 朴素贝叶斯

* ### 优点：
> 在数据较少的情况下仍然有效，可以处理多类别问题。

* ### 缺点：
> 对输入数据的准备方式较为敏感

* ### 适用数据范围：
> 标称型

* ### 工作原理：
> 选择具有最高概率的决策。
>
> 对于分为两类，若p1(x,y)>p2(x,y),那么属于类别1，反之属于类别2
>
> 以上为简化描述
>
> 真正需要计算的是p(c1|x,y)和p(c2|x,y),含义是给定某个x,y表示的数据点，那么该数据点来自类别c1的概率是多少，来自类别c2的概率是多少
>
> 可以用贝叶斯准则来交换概率中的条件与结果
>
> p(ci|x,y) = p(x,y|ci)p(ci)/p(x,y)
>
> p(x,y|ci)代表ci条件中符合x,y的概率

* ### 操作步骤:
>
>（1）收集数据：本案例使用RSS源；
>
>（2）准备数据：需要数值型或者布尔型数据；
>
>（3）分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好；
>
>（4）训练算法：计算不同的独立特征的条件概率；
>
>（5）测试算法：计算错误率。
>
>（6）使用算法：文档分类。

* ### 朴素贝叶斯:
> 如果每个特征需要N个样本，那么1000个特征的词汇表需要N^1000个样本
>
> 如果特征之间相互独立，那么样本数就可以从N^1000减少到1000*N，因为在条件概率中可以把这些概率直接相乘。
>
> 朴素的含义就是假设特征间独立，没有关系。另一假设就是每个特征同等重要（实现方法中只考虑出现与否，不考虑出现次数）。


* ### 案例1:使用Python进行文本分类
> 先准备数据，从文本中构建词向量。
>
> > 1.先将词条去掉标点后切分为词条集合。同时返回集合们的标签（例如是否是侮辱性词汇）
> >
> > 2.创建一个包含在所有文档中出现的不重复词的列表。
> >
> > 3.查看词汇列表中的单词是否出现在想要检查（输入）的文档中。等到一个词汇表单词是否出现的list。
> >
> 训练算法，从词向量计算概率
>
> > 伪代码：
> >
> > 计算每个类别中文档数目
> >
> > 对每篇训练文档：
> >
> >    对每个类别：
> >
> >        如果此条出现在文档中---增加该词条的计数值
> >
> >        增加所有词条的计数值
> >
> > 对每个类别：
> >
> >    对每个词条：
> >
> >        将该词条的数目除以总词条数目得到条件概率
> >
> > 返回每个类别的条件概率